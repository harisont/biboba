@misc{multigec-resource,
	title        = {{MultiGEC} [dataset]},
	author       = {Masciolini, Arianna and Caines, Andrew and De Clercq, Orph{\'e}e and Kruijsbergen, Joni and Kurfal{\i}, Murathan and Mu{\~n}oz S{\'a}nchez, Ricardo and Volodina, Elena and Östling, Robert and Allkivi, Kais and Arhar Holdt, {\v{S}}pela and Auzina, Ilze and Dar{\`g}is, Roberts and Drakonaki, Elena and Frey, Jennifer-Carmen and Gli{\v{s}}i{\'c}, Isidora and Kikilintza, Pinelopi and Nicolas, Lionel and Romanyshyn, Mariana and Rosen, Alexandr and Rozovskaya, Alla and Suluste, Kristjan and Syvokon, Oleksiy and Tantos, Alexandros and Touriki, Despoina-Ourania and Tsiotskas, Konstantinos and Tsourilla, Eleni and Varsamopoulos, Vassilis and Wisniewski, Katrin and {\v{Z}}agar, Ale{\v{s}} and Zesch, Torsten},
	year         = 2025,
	publisher    = {Språkbanken Text},
	doi          = {10.23695/h9f5-8143},
	url          = {https://spraakbanken.gu.se/resurser/multigec},
	note         = {Distributed by Språkbanken Text. PID \url{https://doi.org/10.23695/h9f5-8143}},
	keywords     = {Language Technology (Computational Linguistics), grammatical error correction, language learning, essays, multilinguality},
	language     = {ces, deu, ell, eng, est, isl, ita, lav, rus, slv, swe, ukr},
}

@inproceedings{multigec-state,
	title        = {An overview of {G}rammatical {E}rror {C}orrection for the twelve {MultiGEC-2025} languages},
	author       = {Masciolini, Arianna and Caines, Andrew and De Clercq, Orph{\'e}e and Kruijsbergen, Joni and Kurfal{\i}, Murathan and Mu{\~n}oz S{\'a}nchez, Ricardo and Volodina, Elena and Östling, Robert and Allkivi, Kais and Arhar Holdt, {\v{S}}pela and Auzina, Ilze and Dar{\`g}is, Roberts and Drakonaki, Elena and Frey, Jennifer-Carmen and Gli{\v{s}}i{\'c}, Isidora and Kikilintza, Pinelopi and Nicolas, Lionel and Romanyshyn, Mariana and Rosen, Alexandr and Rozovskaya, Alla and Suluste, Kristjan and Syvokon, Oleksiy and Tantos, Alexandros and Touriki, Despoina-Ourania and Tsiotskas, Konstantinos and Tsourilla, Eleni and Varsamopoulos, Vassilis and Wisniewski, Katrin and {\v{Z}}agar, Ale{\v{s}} and Zesch, Torsten},
	year         = 2025,
	publisher    = {Institution for Swedish, Multilingualism, Language Technology; University of Gothenburg},
	address      = {Gothenburg, Sweden},
	url          = {https://hdl.handle.net/2077/84800},
}

@article{multigec-paper,
	title        = {Towards better language representation in {N}atural {L}anguage {P}rocessing -- a multilingual dataset for text-level {G}rammatical {E}rror {C}orrection},
	author       = {Masciolini, Arianna and Caines, Andrew and De Clercq, Orph{\'e}e and Kruijsbergen, Joni and Kurfal{\i}, Murathan and Mu{\~n}oz S{\'a}nchez, Ricardo and Volodina, Elena and Östling, Robert and Allkivi, Kais and Arhar Holdt, {\v{S}}pela and Auzina, Ilze and Dar{\`g}is, Roberts and Drakonaki, Elena and Frey, Jennifer-Carmen and Gli{\v{s}}i{\'c}, Isidora and Kikilintza, Pinelopi and Nicolas, Lionel and Romanyshyn, Mariana and Rosen, Alexandr and Rozovskaya, Alla and Suluste, Kristjan and Syvokon, Oleksiy and Tantos, Alexandros and Touriki, Despoina-Ourania and Tsiotskas, Konstantinos and Tsourilla, Eleni and Varsamopoulos, Vassilis and Wisniewski, Katrin and {\v{Z}}agar, Ale{\v{s}} and Zesch, Torsten},
	year         = 2025,
	journal      = {International Journal of Learner Corpus Research},
	publisher    = {John Benjamins Publishing Company Amsterdam/Philadelphia},
	volume       = {},
	pages        = {},
  url          = {https://doi.org/10.1075/ijlcr.24033.mas}
}

@inproceedings{multigec-2025,
    title = "The {M}ulti{GEC}-2025 Shared Task on {M}ultilingual {G}rammatical {E}rror {C}orrection at {NLP}4{CALL}",
    author = {Masciolini, Arianna  and
      Caines, Andrew  and
      Clercq, Orph{\'e}e De  and
      Kruijsbergen, Joni  and
      Kurfal{\i}, Murathan  and
      S{\'a}nchez, Ricardo Mu{\~n}oz  and
      Volodina, Elena  and
      {\"O}stling, Robert},
    editor = "S{\'a}nchez, Ricardo Mu{\~n}oz  and
      Alfter, David  and
      Volodina, Elena  and
      Kallas, Jelena",
    booktitle = "Proceedings of the 14th Workshop on Natural Language Processing for Computer Assisted Language Learning",
    month = mar,
    year = "2025",
    address = "Tallinn, Estonia",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2025.nlp4call-1.1/",
    pages = "1--33",
    ISBN = "978-9908-53-112-0"
}

@inproceedings{multigec-2025-uamcsi,
    title = "{UAM}-{CSI} at {M}ulti{GEC}-2025: Parameter-efficient {LLM} Fine-tuning for Multilingual Grammatical Error Correction",
    author = "Staruch, Ryszard",
    editor = "S{\'a}nchez, Ricardo Mu{\~n}oz  and
      Alfter, David  and
      Volodina, Elena  and
      Kallas, Jelena",
    booktitle = "Proceedings of the 14th Workshop on Natural Language Processing for Computer Assisted Language Learning",
    month = mar,
    year = "2025",
    address = "Tallinn, Estonia",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2025.nlp4call-1.3/",
    pages = "42--49",
    ISBN = "978-9908-53-112-0"
}

@inproceedings{multigec-2025-lattice,
    title = "Lattice @{M}ulti{GEC}-2025: A Spitful Multilingual Language Error Correction System Using {LL}a{MA}",
    author = "Seminck, Olga  and
      Dupont, Yoann  and
      Dehouck, Mathieu  and
      Wang, Qi  and
      Durandard, No{\'e}  and
      Novikov, Margo",
    editor = "S{\'a}nchez, Ricardo Mu{\~n}oz  and
      Alfter, David  and
      Volodina, Elena  and
      Kallas, Jelena",
    booktitle = "Proceedings of the 14th Workshop on Natural Language Processing for Computer Assisted Language Learning",
    month = mar,
    year = "2025",
    address = "Tallinn, Estonia",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2025.nlp4call-1.2/",
    pages = "34--41",
    ISBN = "978-9908-53-112-0"
}

@article{multigec-thesis-ud,
  title={{G}rammatical {E}rror {C}orrection Using {L}arge {L}anguage {M}odels: A Case Study on {U}niversal {D}ependencies Treebanks},
  author={Jalali, Arvin},
  year={2025}
}

@article{multigec-ukrainian,
  title={Multilingual {G}rammatical {E}rror {C}orrection with {L}arge {L}anguage {M}odels},
  author={Kovalchuk, Roman and Romanyshyn, Mariana and Ivaniuk, Petro},
  year={2025}
}

@article{multigec-thesis-futark,
  title={From {F}uþark to essay - How well does the {V}iking {LLM} perform {G}rammatical {E}rror {C}orrection?},
  author={Karl Törnblom Bartholf},
  year={2025}
}

@inproceedings{kovalchuk-etal-2025-introducing,
  title = {Introducing {O}mni{GEC}: A Silver Multilingual Dataset for Grammatical Error Correction},
  author = {Kovalchuk, Roman and Romanyshyn, Mariana and Ivaniuk, Petro},
  editor = {Romanyshyn, Mariana},
  booktitle = {Proceedings of the Fourth Ukrainian Natural Language Processing Workshop (UNLP 2025)},
  month = {jul},
  year = {2025},
  address = {Vienna, Austria (online)},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2025.unlp-1.17/},
  pages = {162--178},
  isbn = {979-8-89176-269-5},
  abstract = {In this paper, we introduce OmniGEC, a collection of multilingual silver-standard datasets for the task of Grammatical Error Correction (GEC), covering eleven languages: Czech, English, Estonian, German, Greek, Icelandic, Italian, Latvian, Slovene, Swedish, and Ukrainian. These datasets facilitate the development of multilingual GEC solutions and help bridge the data gap in adapting English GEC solutions to multilingual GEC. The texts in the datasets originate from three sources: Wikipedia edits for the eleven target languages, subreddits from Reddit in the eleven target languages, and the Ukrainian-only UberText 2.0 social media corpus. While Wikipedia edits were derived from human-made corrections, the Reddit and UberText 2.0 data were automatically corrected with the GPT-4o-mini model. The quality of the corrections in the datasets was evaluated both automatically and manually. Finally, we fine-tune two open-source large language models {---} Aya-Expanse (8B) and Gemma-3 (12B) {---} on the multilingual OmniGEC corpora and achieve state-of-the-art (SOTA) results for paragraph-level multilingual GEC. The dataset collection and the best-performing models are available on Hugging Face.},
}

@misc{qiu2025multilingualgrammaticalerrorannotation,
  title = {Multilingual Grammatical Error Annotation: Combining Language-Agnostic Framework with Language-Specific Flexibility},
  author = {Mengyang Qiu and Tran Minh Nguyen and Zihao Huang and Zelong Li and Yang Gu and Qingyu Gao and Siliang Liu and Jungyeul Park},
  year = {2025},
  eprint = {2506.07719},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  url = {https://arxiv.org/abs/2506.07719},
}

@inproceedings{ruppenhofer-etal-2025-annotating,
  title = {Where it{'}s at: Annotating Verb Placement Types in Learner Language},
  author = {Ruppenhofer, Josef and Annette Portmann, Annette and Renker, Christine and Schwendemann, Matthias and Wisniewski, Katrin and Zesch, Torsten},
  editor = {Peng, Siyao and Rehbein, Ines},
  booktitle = {Proceedings of the 19th Linguistic Annotation Workshop (LAW-XIX-2025)},
  month = {jul},
  year = {2025},
  address = {Vienna, Austria},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2025.law-1.15/},
  pages = {187--200},
  isbn = {979-8-89176-262-6},
  abstract = {The annotation of learner language is an often ambiguous and challenging task. It is therefore surprising that in Second Language Acquisition research, information on annotation quality is hardly ever published. This is also true for verb placement, a linguistic feature that has re- ceived much attention within SLA. This paper presents an annotation on verb placement in German learner texts at different proficiency levels. We argue that as part of the annotation process target hypotheses should be provided as ancillary annotations that make explicit each annotator{'}s interpretation of a learner sentence. Our study demonstrates that verb placement can be annotated with high agreement between multiple annotators, for texts at all proficiency levels and across sentences of varying complex- ity. We release our corpus with annotations by four annotators on more than 600 finite clauses sampled across 5 CEFR levels.},
}

@inproceedings{ostling-etal-2025-llm,
  title = {{LLM}-based post-editing as reference-free {GEC} evaluation},
  author = {{\"O}stling, Robert and Kurfali, Murathan and Caines, Andrew},
  editor = {Kochmar, Ekaterina and Alhafni, Bashar and Bexte, Marie and Burstein, Jill and Horbach, Andrea and Laarmann-Quante, Ronja and Tack, Ana{\"i}s and Yaneva, Victoria and Yuan, Zheng},
  booktitle = {Proceedings of the 20th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2025)},
  month = {jul},
  year = {2025},
  address = {Vienna, Austria},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2025.bea-1.16/},
  pages = {213--224},
  isbn = {979-8-89176-270-1},
  abstract = {Evaluation of Grammatical Error Correction (GEC) systems is becoming increasingly challenging as the quality of such systems increases and traditional automatic metrics fail to adequately capture such nuances as fluency versus minimal edits, alternative valid corrections compared to the `ground truth', and the difference between corrections that are useful in a language learning scenario versus those preferred by native readers. Previous work has suggested using human post-editing of GEC system outputs, but this is very labor-intensive. We investigate the use of Large Language Models (LLMs) as post-editors of English and Swedish texts, and perform a meta-analysis of a range of different evaluation setups using a set of recent GEC systems. We find that for the two languages studied in our work, automatic evaluation based on post-editing agrees well with both human post-editing and direct human rating of GEC systems. Furthermore, we find that a simple n-gram overlap metric is sufficient to measure post-editing distance, and that including human references when prompting the LLMs generally does not improve agreement with human ratings. The resulting evaluation metric is reference-free and requires no language-specific training or additional resources beyond an LLM capable of handling the given language.Evaluation of Grammatical Error Correction (GEC) systems is becoming increasingly challenging as the quality of such systems increases and traditional automatic metrics fail to adequately capture such nuances as fluency versus minimal edits, alternative valid corrections compared to the `ground truth', and the difference between corrections that are useful in a language learning scenario versus those preferred by native readers. Previous work has suggested using human post-editing of GEC system outputs, but this is very labor-intensive. We investigate the use of Large Language Models (LLMs) as post-editors of English and Swedish texts, and perform a meta-analysis of a range of different evaluation setups using a set of recent GEC systems. We find that for the two languages studied in our work, automatic evaluation based on post-editing agrees well with both human post-editing and direct human rating of GEC systems. Furthermore, we find that a simple n-gram overlap metric is sufficient to measure post-editing distance, and that including human references when prompting the LLMs generally does not improve agreement with human ratings. The resulting evaluation metric is reference-free and requires no language-specific training or additional resources beyond an LLM capable of handling the given language.},
}

@inproceedings{vainikko-etal-2025-paragraph,
  title = {Paragraph-level Error Correction and Explanation Generation: Case Study for {E}stonian},
  author = {Vainikko, Martin and Kamarik, Taavi and Kert, Karina and Liin, Krista and Maine, Silvia and Allkivi, Kais and Kaivapalu, Annekatrin and Fishel, Mark},
  editor = {Kochmar, Ekaterina and Alhafni, Bashar and Bexte, Marie and Burstein, Jill and Horbach, Andrea and Laarmann-Quante, Ronja and Tack, Ana{\"i}s and Yaneva, Victoria and Yuan, Zheng},
  booktitle = {Proceedings of the 20th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2025)},
  month = {jul},
  year = {2025},
  address = {Vienna, Austria},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2025.bea-1.72/},
  pages = {953--967},
  isbn = {979-8-89176-270-1},
  abstract = {We present a case study on building task-specific models for grammatical error correction and explanation generation tailored to learners of Estonian. Our approach handles whole paragraphs instead of sentences and leverages prompting proprietary large language models for generating synthetic training data, addressing the limited availability of error correction data and the complete absence of correction justification/explanation data in Estonian. We describe the chosen approach and pipeline and provide technical details for the experimental part. The final outcome is a set of open-weight models, which are released with a permissive license along with the generated synthetic error correction and explanation data.},
}

@misc{piotrowska2025multilingual,
  title = {Multilingual document-level GEC evaluation},
  author = {Piotrowska, Emilia},
  year = {2025},
}