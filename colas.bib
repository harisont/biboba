@article{cola,
  title = {Neural Network Acceptability Judgments},
  author = {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  journal = {arXiv preprint arXiv:1805.12471},
  year = {2018},
}

@inproceedings{nocola,
  title = {{N}o{C}o{LA}: The {N}orwegian Corpus of Linguistic Acceptability},
  author = {Jentoft, Matias and Samuel, David},
  editor = {Alum{\"a}e, Tanel and Fishel, Mark},
  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},
  month = {may},
  year = {2023},
  address = {T{\'o}rshavn, Faroe Islands},
  publisher = {University of Tartu Library},
  url = {https://aclanthology.org/2023.nodalida-1.60},
  pages = {610--617},
  abstract = {While there has been a surge of large language models for Norwegian in recent years, we lack any tool to evaluate their understanding of grammaticality. We present two new Norwegian datasets for this task. NoCoLA-class is a supervised binary classification task where the goal is to discriminate between acceptable and non-acceptable sentences. On the other hand, NoCoLA-zero is a purely diagnostic task for evaluating the grammatical judgement of a language model in a completely zero-shot manner, i.e. without any further training. In this paper, we describe both datasets in detail, show how to use them for different flavors of language models, and conduct a comparative study of the existing Norwegian language models.},
}

@inproceedings{rucola,
  title = {{R}u{C}o{LA}: {R}ussian Corpus of Linguistic Acceptability},
  author = {Mikhailov, Vladislav and Shamardina, Tatiana and Ryabinin, Max and Pestova, Alena and Smurov, Ivan and Artemova, Ekaterina},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  month = {dec},
  year = {2022},
  address = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2022.emnlp-main.348},
  doi = {10.18653/v1/2022.emnlp-main.348},
  pages = {5207--5227},
  abstract = {Linguistic acceptability (LA) attracts the attention of the research community due to its many uses, such as testing the grammatical knowledge of language models and filtering implausible texts with acceptability classifiers.However, the application scope of LA in languages other than English is limited due to the lack of high-quality resources.To this end, we introduce the Russian Corpus of Linguistic Acceptability (RuCoLA), built from the ground up under the well-established binary LA approach. RuCoLA consists of 9.8k in-domain sentences from linguistic publications and 3.6k out-of-domain sentences produced by generative models. The out-of-domain set is created to facilitate the practical use of acceptability for improving language generation.Our paper describes the data collection protocol and presents a fine-grained analysis of acceptability classification experiments with a range of baseline approaches.In particular, we demonstrate that the most widely used language models still fall behind humans by a large margin, especially when detecting morphological and semantic errors. We release RuCoLA, the code of experiments, and a public leaderboard to assess the linguistic competence of language models for Russian.},
}

@inproceedings{itacola,
  title = {Monolingual and Cross-Lingual Acceptability Judgments with the {I}talian {C}o{LA} corpus},
  author = {Trotta, Daniela and Guarasci, Raffaele and Leonardelli, Elisa and Tonelli, Sara},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
  month = {nov},
  year = {2021},
  address = {Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2021.findings-emnlp.250},
  doi = {10.18653/v1/2021.findings-emnlp.250},
  pages = {2929--2940},
  abstract = {The development of automated approaches to linguistic acceptability has been greatly fostered by the availability of the English CoLA corpus, which has also been included in the widely used GLUE benchmark. However, this kind of research for languages other than English, as well as the analysis of cross-lingual approaches, has been hindered by the lack of resources with a comparable size in other languages. We have therefore developed the ItaCoLA corpus, containing almost 10,000 sentences with acceptability judgments, which has been created following the same approach and the same steps as the English one. In this paper we describe the corpus creation, we detail its content, and we present the first experiments on this new resource. We compare in-domain and out-of-domain classification, and perform a specific evaluation of nine linguistic phenomena. We also present the first cross-lingual experiments, aimed at assessing whether multilingual transformer-based approaches can benefit from using sentences in two languages during fine-tuning.},
}

@inproceedings{dalaj,
  title = {{D}a{LAJ} {--} a dataset for linguistic acceptability judgments for {S}wedish},
  author = {Volodina, Elena and Mohammed, Yousuf Ali and Klezl, Julia},
  editor = {Alfter, David and Volodina, Elena and Pilan, Ildik{\'o} and Gra{\"e}n, Johannes and Borin, Lars},
  booktitle = {Proceedings of the 10th Workshop on NLP for Computer Assisted Language Learning},
  month = {may},
  year = {2021},
  address = {Online},
  publisher = {LiU Electronic Press},
  url = {https://aclanthology.org/2021.nlp4call-1.3},
  pages = {28--37},
  tags = {content-data, lang-sv, type-paper},
}

@inproceedings{dalaj-ged,
  title = {{D}a{LAJ}-{GED} - a dataset for Grammatical Error Detection tasks on {S}wedish},
  author = {Volodina, Elena and Mohammed, Yousuf Ali and Berdicevskis, Aleksandrs and Bouma, Gerlof and {\"O}hman, Joey},
  editor = {Alfter, David and Volodina, Elena and Fran{\c{c}}ois, Thomas and J{\"o}nsson, Arne and Rennes, Evelina},
  booktitle = {Proceedings of the 12th Workshop on NLP for Computer Assisted Language Learning},
  month = {may},
  year = {2023},
  address = {T{\'o}rshavn, Faroe Islands},
  publisher = {LiU Electronic Press},
  url = {https://aclanthology.org/2023.nlp4call-1.11},
  pages = {94--101},
}

@article{blimp,
  title = {{BL}i{MP}: The Benchmark of Linguistic Minimal Pairs for {E}nglish},
  author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
  editor = {Johnson, Mark and Roark, Brian and Nenkova, Ani},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  year = {2020},
  address = {Cambridge, MA},
  publisher = {MIT Press},
  url = {https://aclanthology.org/2020.tacl-1.25},
  doi = {10.1162/tacl_a_00321},
  pages = {377--392},
  abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs{---}that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4{\%}. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.},
}

@article{jcola,
  title = {{JCoLA}: {J}apanese {C}orpus of {L}inguistic {A}cceptability},
  author = {Someya, Taiga and Sugimoto, Yushi and Oseki, Yohei},
  journal = {arXiv preprint arXiv:2309.12676},
  year = {2023},
}

@inproceedings{escola,
  title = {{E}s{C}o{LA}: {S}panish Corpus of Linguistic Acceptability},
  author = {Bel, Nuria and Punsola, Marta and Ru{\'\i}z-Fern{\'a}ndez, Valle},
  editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  month = {may},
  year = {2024},
  address = {Torino, Italia},
  publisher = {ELRA and ICCL},
  url = {https://aclanthology.org/2024.lrec-main.554},
  pages = {6268--6277},
  abstract = {Acceptability is one of the General Language Understanding Evaluation Benchmark (GLUE) probing tasks proposed to assess the linguistic capabilities acquired by a deep-learning transformer-based language model (LM). In this paper, we introduce the Spanish Corpus of Linguistic Acceptability EsCoLA. EsCoLA has been developed following the example of other linguistic acceptability data sets for English, Italian, Norwegian or Russian, with the aim of having a complete GLUE benchmark for Spanish. EsCoLA consists of 11,174 sentences and their acceptability judgements as found in well-known Spanish reference grammars. Additionally, all sentences have been annotated with the class of linguistic phenomenon the sentence is an example of, also following previous practices. We also provide as task baselines the results of fine-tuning four different language models with this data set and the results of a human annotation experiment. Results are also analyzed and commented to guide future research. EsCoLA is released under a CC-BY 4.0 license and freely available at https://doi.org/10.34810/data1138.},
}

@article{catcola,
  title = {{CatCoLA}, {C}atalan {C}orpus of {L}inguistic {A}cceptability},
  author = {Bel, N{\'u}ria and Punsola, Marta and Ruiz-Fern{\'a}ndez, Valle},
  year = {2024},
  journal = {Procesamiento del Lenguaje Natural},
  volume = {73},
  pages = {177--190},
  tags = {lang-ca, type-paper, content-data},
}

@article{multiblimp,
  title = {MultiBLiMP 1.0: A massively multilingual benchmark of linguistic minimal pairs},
  author = {Jumelet, Jaap and Weissweiler, Leonie and Nivre, Joakim and Bisazza, Arianna},
  journal = {arXiv preprint arXiv:2504.02768},
  year = {2025},
}